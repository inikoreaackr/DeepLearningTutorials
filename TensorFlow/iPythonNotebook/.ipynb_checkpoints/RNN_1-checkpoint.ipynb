{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent neural network(RNN) \n",
    "\n",
    "\n",
    "Recurrent neural network(RNN)은 Text, 음성, 시계열 데이터등 순차적인(Sequential) 데이터들에 대한 분석을 할때 주로 사용하는 알고리즘입니다. <br>\n",
    "본 문서에서는 RNN의 기본적인 내용부터 실습하겠습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-13T22:30:39.473848Z",
     "start_time": "2017-08-13T22:30:29.580444Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#-*- coding: utf-8 -*-\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib import rnn\n",
    "import pprint #데이터를 보기 좋게 출력해주는 모듈"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text 데이터를 어떻게 Neural Network에 넣을까?\n",
    "Text 데이터를 Neural Network에 넣는 방법은 벡터화(embedding)입니다.<br>\n",
    "Text 데이터를 벡터화 시키는 방법은 여러가지(One-hot, bag of words, tf-idf, word embedding)가 있습니다.<br>\n",
    "오늘은 그중 하나인 one-hot representation을 사용하겠습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-13T22:30:39.480953Z",
     "start_time": "2017-08-13T22:30:39.475443Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 각 문자를 벡터로 나타내기 위해 one-hot representation 형태로 나타냅니다.\n",
    "# I LOVE YOU 를 표현하기 위해 문자를 One-hot represnetation으로 나타내었습니다.\n",
    "I = [1, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "L = [0, 1, 0, 0, 0, 0, 0]\n",
    "O = [0, 0, 1, 0, 0, 0, 0]\n",
    "V = [0, 0, 0, 1, 0, 0, 0]\n",
    "E = [0, 0, 0, 0, 1, 0, 0]\n",
    "\n",
    "Y = [0, 0, 0, 0, 0, 1, 0]\n",
    "U = [0, 0, 0, 0, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-13T22:30:39.487539Z",
     "start_time": "2017-08-13T22:30:39.482412Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN 과 LSTM\n",
    "Tensorflow에서는 쉽게 RNN을 정의 할 수 있습니다. 아래에서는 편의상 함수의 형태로 정의하였습니다.<br>\n",
    "아래 선언된 RNN cell중 LSTM(Long Short-Term Memory)은 RNN의 Vanishing gradient 문제를 해결하고 이전 정보를 더욱 잘 반영하기 위해서 RNN의 Activation unit을 변경한 것입니다.\n",
    "![lstm](./assets/lstm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-13T22:30:39.514348Z",
     "start_time": "2017-08-13T22:30:39.489457Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def RNN(x_input, sequence_length, rnn_type='rnn'):\n",
    "    with tf.variable_scope('rnn') as scope:\n",
    "        \n",
    "        hidden_size = 2 # RNN의 Hidden Layer의 차원을 의미합니다.\n",
    "        \n",
    "        cell = None\n",
    "        if(rnn_type == 'rnn'):\n",
    "            cell = tf.nn.rnn_cell.BasicRNNCell(num_units=hidden_size)\n",
    "        elif(rnn_type == 'lstm'):\n",
    "            cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=hidden_size, state_is_tuple=True)\n",
    "            #state_is_tuple = True로 하면 State의 값을 tuple형태로 분리해서 보여줍니다.\n",
    "                                    \n",
    "        outputs, states = tf.nn.dynamic_rnn(cell, x_input,\n",
    "                                            sequence_length=sequence_length, #[8], ILOVEYOU의 길이\n",
    "                                            dtype=tf.float32)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print(\"outputs of RNN\")\n",
    "        print(sess.run(outputs))\n",
    "        print(\"states of RNN\")\n",
    "        pprint.pprint(sess.run(states))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow에서 데이터의 가장 첫번째 차원은 Batch_size !\n",
    "Tensorflow에서 데이터의 가장 첫번째 Dimension은 Batch에 해당하기 때문에 아래와 같이 사용할 데이터인<br>\n",
    "1차원에 속한 [I, L, O, V, E, Y, O, U]를 -> 2차원인 [[I, L, O, V, E, Y, O, U]]로 할당해 줍니다.<br>\n",
    "현재 차원 (batch_size, sequence_length, one_hot_representation_dimension) == (1, 8, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-13T22:30:39.527469Z",
     "start_time": "2017-08-13T22:30:39.516177Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_input.shape\n",
      "(1, 8, 7)\n",
      "x_input\n",
      "[[[ 1.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  1.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  1.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  1.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  1.  0.]\n",
      "  [ 0.  0.  1.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  1.]]]\n",
      "sequence_length\n",
      "[8]\n"
     ]
    }
   ],
   "source": [
    "x_input = np.array([[I, L, O, V, E, Y, O, U]], dtype=np.float32) \n",
    "print(\"x_input.shape\")\n",
    "print(x_input.shape)\n",
    "print(\"x_input\")\n",
    "print(x_input)\n",
    "sequence_length = x_input.shape[1] #글자 길이\n",
    "sequence_length = [sequence_length] # 첫번째 차원이 Batch기 때문에 실수가 아닌 []로 변환\n",
    "print(\"sequence_length\")\n",
    "print(sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 선언한 RNN함수를 호출\n",
    "RNN함수를 호출합니다. <br>\n",
    "RNN의 각 Time step 별로 어떤 Output이 나오는지, RNN의 State값은 무엇인지를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-13T22:30:39.845341Z",
     "start_time": "2017-08-13T22:30:39.529413Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs of RNN\n",
      "[[[-0.27907825 -0.62655085]\n",
      "  [-0.24744262  0.35407287]\n",
      "  [-0.34570792 -0.44406343]\n",
      "  [-0.20299451  0.06616659]\n",
      "  [ 0.64090687  0.03075653]\n",
      "  [ 0.46430007 -0.53836608]\n",
      "  [-0.83729082 -0.75931978]\n",
      "  [-0.69254398  0.74065953]]]\n",
      "states of RNN\n",
      "array([[-0.69254398,  0.74065953]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "RNN(x_input, sequence_length, rnn_type='rnn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic_RNN\n",
    "텐서플로우에서는 Static_RNN과 Dynamic_RNN이 있습니다. <br>\n",
    "그러나 거의 대부분 Dynamic_RNN을 사용합니다. Dynamic_RNN은 Input data의 길이(sequence)를 고려하여 계산이 가능합니다.<br>\n",
    "아래 코드에서 길이가 8인 문자열에 대해서 길이 7만큼만 계산하면 결과가 어떻게 나오는지 확인해보시면 7이상인 경우 Output값이 0이 나옴을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-13T22:30:40.345592Z",
     "start_time": "2017-08-13T22:30:39.846931Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs of RNN\n",
      "[[[ 0.10606093 -0.07082008]\n",
      "  [-0.03770325 -0.12678891]\n",
      "  [ 0.00350977 -0.05389421]\n",
      "  [-0.10034209 -0.07521853]\n",
      "  [-0.11093281 -0.12175631]\n",
      "  [-0.1016638  -0.01515914]\n",
      "  [-0.03943169  0.03463318]\n",
      "  [ 0.04629521 -0.12834823]]\n",
      "\n",
      " [[ 0.10606093 -0.07082008]\n",
      "  [-0.03770325 -0.12678891]\n",
      "  [ 0.00350977 -0.05389421]\n",
      "  [-0.10034209 -0.07521853]\n",
      "  [-0.11093281 -0.12175631]\n",
      "  [-0.1016638  -0.01515914]\n",
      "  [-0.03943169  0.03463318]\n",
      "  [ 0.          0.        ]]]\n",
      "states of RNN\n",
      "LSTMStateTuple(c=array([[ 0.08968972, -0.25370511],\n",
      "       [-0.10609682,  0.06283452]], dtype=float32), h=array([[ 0.04629521, -0.12834823],\n",
      "       [-0.03943169,  0.03463318]], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "# Sequence Length를 3으로 변경하면 Dynamic RNN의 특성상\n",
    "# Time Step 3번째 까지만 계산합니다.\n",
    "#tf.get_variable_scope().reuse_variables() # 같은 RNN을 재사용할때 필요한 코드\n",
    "x_input_2 = np.array([[I, L, O, V, E, Y, O, U],\n",
    "                      [I, L, O, V, E, Y, O, U]], dtype=np.float32) \n",
    "RNN(x_input_2, sequence_length=[8,7], rnn_type='lstm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"I LOVE YOU\" 문장의 길이가 총 8이므로 RNN의 Output도 각 Time Step별로 총 8개가 나옵니다. <br>RNN의 State는 같은 Neural Network를 반복적으로 쓰는 RNN의 특성 때문에 Time Step과 상관없이 하나의 값이 나오게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer RNN\n",
    "RNN을 여러개 쌓을 수도 있습니다. TensorFlow에서는 tf.contrib.rnn.MultiRNNCell이라는 API를 제공하며 아래와 같이 Multi Layer RNN을 구성할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-13T22:30:40.398292Z",
     "start_time": "2017-08-13T22:30:40.347617Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Multi_RNN(x_input, sequence_length, rnn_type='rnn'):\n",
    "    with tf.variable_scope('multi_rnn') as scope:\n",
    "        \n",
    "        hidden_size = 2\n",
    "        num_layers = 2\n",
    "        \n",
    "        cell = None\n",
    "        if(rnn_type == 'rnn'):\n",
    "            cell = [tf.contrib.rnn.BasicRNNCell(num_units=hidden_size) for i in range(num_layers)]\n",
    "        elif(rnn_type == 'lstm'):\n",
    "            cell = [tf.contrib.rnn.BasicLSTMCell(num_units=hidden_size, state_is_tuple=True) for i in range(num_layers)]\n",
    "            #state_is_tuple = True로 하면 Cell State와 Hidden State의 값을 tuple형태로 분리해서 보여줍니다.\n",
    "        cell = tf.contrib.rnn.MultiRNNCell(cells=cell, state_is_tuple=True)\n",
    "        outputs, states = tf.nn.dynamic_rnn(cell, x_input,\n",
    "                                            sequence_length=sequence_length,\n",
    "                                            dtype=tf.float32)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print(\"outputs of Multi_RNN\")\n",
    "        print(sess.run(outputs))\n",
    "        print(\"states of Multi_RNN\")\n",
    "        pprint.pprint(sess.run(states))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi Layer RNN의 경우 Output값은 가장 위에 층의 값을 사용하게 됩니다.\n",
    "State는 각 층당 1개씩 출력해주는걸 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-13T22:30:41.106165Z",
     "start_time": "2017-08-13T22:30:40.402784Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs of Multi_RNN\n",
      "[[[ 0.01167961  0.02644865]\n",
      "  [ 0.01166231  0.03115742]\n",
      "  [ 0.02100151  0.04661337]\n",
      "  [ 0.0291939   0.07342213]\n",
      "  [ 0.00987768  0.0653971 ]\n",
      "  [-0.00769255  0.05260058]\n",
      "  [-0.00775615  0.05054199]\n",
      "  [-0.00630805  0.04451147]]]\n",
      "states of Multi_RNN\n",
      "(LSTMStateTuple(c=array([[-0.18114896, -0.19035339]], dtype=float32), h=array([[-0.07953743, -0.10639531]], dtype=float32)),\n",
      " LSTMStateTuple(c=array([[-0.01243851,  0.0895907 ]], dtype=float32), h=array([[-0.00630805,  0.04451147]], dtype=float32)))\n"
     ]
    }
   ],
   "source": [
    "Multi_RNN(x_input, sequence_length, rnn_type='lstm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Bidirectional RNN\n",
    "빈칸 넣기 게임처럼 문맥을 고려해야 해결할 수 있을 때가 있습니다. <br>\n",
    "Bidirectional RNN은 이러한 경우를 고려한 뉴럴 네트워크입니다. <br>\n",
    "Tensorflow에서는 tf.nn.bidirectional_dynamic_rnn이라는 API를 통해 손쉽게 Bidirectional RNN을 구성할 수 있습니다. <br>\n",
    "\n",
    "### Tip\n",
    "Bidirectional RNN에 데이터가 들어가는 과정에 대해서 조금 더 자세히 말씀드리자면, Input 데이터의 순서를 '반대'로 바꿔서 집어 넣는다고 생각하시면 됩니다. 이러한 과정은 TensorFlow 내에서 내부적으로 이루어 집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Bi_RNN(x_input, sequence_length, rnn_type='rnn'):\n",
    "    with tf.variable_scope('bidirectional_rnn') as scope:\n",
    "        \n",
    "        hidden_size = 2\n",
    "        num_layers = 2\n",
    "        \n",
    "        cell_fw = None\n",
    "        cell_bw = None\n",
    "        if(rnn_type == 'rnn'):\n",
    "            cell_fw = tf.contrib.rnn.BasicRNNCell(num_units=hidden_size)\n",
    "            cell_bw = tf.contrib.rnn.BasicRNNCell(num_units=hidden_size)\n",
    "        elif(rnn_type == 'lstm'):\n",
    "            cell_fw = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_size, state_is_tuple=True)\n",
    "            cell_bw = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_size, state_is_tuple=True)\n",
    "            #state_is_tuple = True로 하면 Cell State와 Hidden State의 값을 tuple형태로 분리해서 보여줍니다.\n",
    "            \n",
    "        outputs, states = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, x_input, \n",
    "                                                          sequence_length=sequence_length,\n",
    "                                                          dtype=tf.float32)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print(\"outputs of Multi_RNN\")\n",
    "        print(sess.run(outputs))\n",
    "        print(\"states of Multi_RNN\")\n",
    "        pprint.pprint(sess.run(states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs of Multi_RNN\n",
      "(array([[[ 0.0111236 ,  0.09289712],\n",
      "        [ 0.01819379, -0.01025583],\n",
      "        [-0.08650778, -0.00899031],\n",
      "        [-0.19254678,  0.18635565],\n",
      "        [-0.1403967 ,  0.17292154],\n",
      "        [-0.14122301,  0.16089427],\n",
      "        [-0.14163345,  0.08451635],\n",
      "        [-0.1743028 ,  0.10539717]]], dtype=float32), array([[[ 0.02427619,  0.06595678],\n",
      "        [ 0.00637439,  0.11084403],\n",
      "        [ 0.09749091,  0.14135244],\n",
      "        [ 0.05251563,  0.17267756],\n",
      "        [ 0.00207342,  0.10239065],\n",
      "        [-0.04261608,  0.12694027],\n",
      "        [-0.02245589, -0.0126976 ],\n",
      "        [-0.08202109, -0.07719878]]], dtype=float32))\n",
      "states of Multi_RNN\n",
      "(LSTMStateTuple(c=array([[-0.35808173,  0.24626106]], dtype=float32), h=array([[-0.1743028 ,  0.10539717]], dtype=float32)),\n",
      " LSTMStateTuple(c=array([[ 0.038118  ,  0.11336834]], dtype=float32), h=array([[ 0.02427619,  0.06595678]], dtype=float32)))\n"
     ]
    }
   ],
   "source": [
    "Bi_RNN(x_input, sequence_length, rnn_type='lstm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output값과 State값이 데이터를 원래의 방향대로 넣어서(forward) 나온 값과 순서를 반대로 해서 넣은값(backward)값 2개로 나뉘어서 나오게 됩니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer Bidirectional RNN\n",
    "Bidirectional RNN을 여러층 쌓아 Multi Layer Bidirectional RNN을 만들어 보도록 하겠습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Multi_Bi_RNN(x_input, sequence_length, rnn_type='rnn'):\n",
    "    with tf.variable_scope('multi_bidirectional_rnn') as scope:\n",
    "        \n",
    "        hidden_size = 2\n",
    "        num_layers = 2\n",
    "        \n",
    "        cell_fw = None\n",
    "        cell_bw = None\n",
    "        if(rnn_type == 'rnn'):\n",
    "            cell_fw = [tf.contrib.rnn.BasicRNNCell(num_units=hidden_size) for i in range(num_layers)]\n",
    "            cell_bw = [tf.contrib.rnn.BasicRNNCell(num_units=hidden_size) for i in range(num_layers)]\n",
    "            \n",
    "        elif(rnn_type == 'lstm'):\n",
    "            cell_fw = [tf.contrib.rnn.BasicLSTMCell(num_units=hidden_size, state_is_tuple=True) for i in range(num_layers)]\n",
    "            cell_bw = [tf.contrib.rnn.BasicLSTMCell(num_units=hidden_size, state_is_tuple=True) for i in range(num_layers)]\n",
    "            #state_is_tuple = True로 하면 Cell State와 Hidden State의 값을 tuple형태로 분리해서 보여줍니다.\n",
    "        cell_fw = tf.contrib.rnn.MultiRNNCell(cells=cell_fw, state_is_tuple=True)\n",
    "        cell_bw = tf.contrib.rnn.MultiRNNCell(cells=cell_bw, state_is_tuple=True)\n",
    "            \n",
    "        outputs, states = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, x_input, \n",
    "                                                          sequence_length=sequence_length,\n",
    "                                                          dtype=tf.float32)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print(\"outputs of Bidirectional_Multi_RNN\")\n",
    "        print(sess.run(outputs))\n",
    "        print(\"states of Bidirectional_Multi_RNN\")\n",
    "        pprint.pprint(sess.run(states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs of Bidirectional_Multi_RNN\n",
      "(array([[[-0.00634974,  0.00238272],\n",
      "        [-0.01635971,  0.00660519],\n",
      "        [-0.01986992,  0.00932127],\n",
      "        [-0.00819738,  0.00646144],\n",
      "        [-0.01067687,  0.00774106],\n",
      "        [ 0.00522121,  0.00142728],\n",
      "        [ 0.01154147, -0.00261095],\n",
      "        [ 0.02283721, -0.01060603]]], dtype=float32), array([[[-0.04740866, -0.04971042],\n",
      "        [-0.03895395, -0.04243946],\n",
      "        [-0.03591039, -0.03848059],\n",
      "        [-0.0372103 , -0.03888252],\n",
      "        [-0.03628064, -0.04191764],\n",
      "        [-0.0293309 , -0.03881156],\n",
      "        [-0.0134256 , -0.02297332],\n",
      "        [-0.00569773, -0.01026752]]], dtype=float32))\n",
      "states of Bidirectional_Multi_RNN\n",
      "((LSTMStateTuple(c=array([[ 0.29269752,  0.19209731]], dtype=float32), h=array([[ 0.1649372 ,  0.10658178]], dtype=float32)),\n",
      "  LSTMStateTuple(c=array([[ 0.04550727, -0.02003788]], dtype=float32), h=array([[ 0.02283721, -0.01060603]], dtype=float32))),\n",
      " (LSTMStateTuple(c=array([[ 0.67063302,  0.51517504]], dtype=float32), h=array([[ 0.30004516,  0.27802894]], dtype=float32)),\n",
      "  LSTMStateTuple(c=array([[-0.08653754, -0.11444339]], dtype=float32), h=array([[-0.04740866, -0.04971042]], dtype=float32))))\n"
     ]
    }
   ],
   "source": [
    "Multi_Bi_RNN(x_input, sequence_length, rnn_type='lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
